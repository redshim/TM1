install.packages("KoNLP")
import(KoNLP)
require(KoNLP)
library(KoNLP)
useSejongDic()
install.packages("KoNLP",dependencies=TRUE)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre6') # for 64-bit version
install.packages("KoNLP", dependencies = TRUE)
library(rJava)
library(KoNLP)
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_73') # for 64-bit version
library(rJava)
library(KoNLP)
setwd("~")
DIC<-read.csv("positive-words_kor.csv", header=FALSE, stringsAsFactors=FALSE) #긍정사전 로딩
library(KoNLP)
useSejongDic()
f <- file("test33.txt", blocking = F, encoding = "UTF-8")
readLines(f)
txt <- readLines(f)
txt
nouns <- sapply(txt, extractNoun, USE.NAMES = T)
length(nouns)
nouns
nouns <- gsub("", "", nouns)
nouns
nouns <- gsub("지금", "", nouns)
nouns <- gsub("", "", nouns)
nouns
nouns <- gsub("U+200B", "", nouns)
nouns
nouns <- gsub("\"U+200B\"", "", nouns)
nouns
library(tm)
library(KoNLP)
docs <-
c("사랑은 달콤한 꽃이나 그것을 따기 위해서는 무서운 벼랑 끝까지 갈 용기가 있어야 한다.",
"진실한 사랑의 실체는 믿음이다.",
"눈물은 눈동자로 말하는 고결한 언어.",
"친구란 두 사람의 신체에 사는 하나의 영혼이다.",
"흐르는 강물을 잡을수 없다면, 바다가 되어서 기다려라.",
"믿음 소망 사랑 그중에 제일은 사랑이라.",
"가장 소중한 사람은 가장 사랑하는 사람이다.",
"사랑 사랑 사랑")
#편의상 검색어도 넣어준다.
query <- "믿음을 주는 사랑"
names(docs) <- paste("doc", 1:length(docs), sep="")
docs <- c(docs, query=query)
docs.corp <- Corpus(VectorSource(docs))
docs
docs.corp
#색인어 추출함수
konlp_tokenizer <- function(doc){
extractNoun(doc)
}
tdmat <- TermDocumentMatrix(docs.corp, control=list(tokenize=konlp_tokenizer,
weighting = function(x) weightTfIdf(x, TRUE),
wordLengths=c(1,Inf)))
tdmat
tdmatmat
tdmatmat <- as.matrix(tdmat)
tdmatmat
norm_vec <- function(x) {x/sqrt(sum(x^2))}
tdmatmat <- apply(tdmatmat, 2, norm_vec)
tdmatmat
tdmatmat[,9]
tdmatmat[,1:8]
docord <- t(tdmatmat[,9]) %*% tdmatmat[,1:8]
docord
#검색 결과 리스팅
orders <- data.frame(docs=docs[-9],scores=t(docord) ,stringsAsFactors=FALSE)
orders[order(docord, decreasing=T),]
fit <- hclust(dist(t(tdmatmat)), method = "ward")
plclust(fit)
rect.hclust(fit, k = 5)
text <- c("clothing men one jacket portrait necktie adult people outerwear facial_expression menswear confidence outfit business fashion stylish neckwear trendy blazer indoors",
"clothing men one jacket portrait adult necktie confidence facial_expression people outerwear menswear business outfit stylish fashion indoors trendy women music",          "clothing men one jacket portrait necktie adult people outerwear menswear confidence facial_expression outfit business fashion stylish indoors blazer shadow neckwear",         "clothing men one jacket portrait necktie adult people facial_expression outerwear menswear confidence outfit business blazer neckwear stylish fashion indoors actor ",
"clothing men one jacket portrait adult people outerwear necktie facial_expression menswear confidence outfit business fashion blazer stylish music indoors actor")
library(tm)
view <- factor(rep(c("view 1"), each=5))
summary(view)
view
df <- data.frame(text, view, stringsAsFactors=FALSE)
df
summary(df)
df$text
df$view
corpus
corpus <- Corpus(VectorSource(df$text))
corpus
tdm = DocumentTermMatrix(corpus)
tdm
inspect(tdm)
df$text
tdm <- as.matrix(tdm)
tdm
tdm
library(proxy)
cosine_dist_mat <- as.matrix(dist(t(tdm), method = "cosine"))
diag(cosine_dist_mat) <- NA
cosine_dist <- apply(cosine_dist_mat, 2, mean, na.rm=TRUE)
cosine_dist
ave(cosine_dist)
setwd("C:\\R\\TM")
filenames <- list.files(getwd(),pattern="*.txt")
files <- lapply(filenames,readLines)
files
enc2utf8(files)
docs <- Corpus(VectorSource(files))
docs
writeLines(as.character(docs[[30]]))
docs <-tm_map(docs,content_transformer(tolower))
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
docs <-tm_map(docs,content_transformer(tolower))
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, toSpace, "’")
docs <- tm_map(docs, toSpace, "‘")
docs <- tm_map(docs, toSpace, "•")
docs <- tm_map(docs, toSpace, "'")
docs <- tm_map(docs, toSpace, '"')
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, stripWhitespace)
writeLines(as.character(docs[[30]]))
docs <- tm_map(docs,stemDocument)
tm_map
docs <- tm_map(docs, content_transformer(gsub),
pattern = "organiz", replacement = "organ")
docs <- tm_map(docs, content_transformer(gsub),
pattern = "organis", replacement = "organ")
docs <- tm_map(docs, content_transformer(gsub),
pattern = "andgovern", replacement = "govern")
docs <- tm_map(docs, content_transformer(gsub),
pattern = "inenterpris", replacement = "enterpris")
docs <- tm_map(docs, content_transformer(gsub),
pattern = "team-", replacement = "team")
myStopwords <- c("can", "say","one","way","use",
#define and eliminate all custom stopwords
"also","howev","tell","will",
"much","need","take","tend","even",
"like","particular","rather","said",
"might","see","someth","thing","point",
"post","look","right","now","think","‘ve ",
"great","lot")
"‘re ","anoth","put","set","new","good",
docs <- tm_map(docs, removeWords, myStopwords)
"last","never","brief","bit","entir","brief",
"enough","far","earli","away","achiev","draw",
"want","sure","kind","larg","yes,","day","etc",
"first","two","help","often","may",
"get","well","make","ask","come","end",
"littl","ever","moreov","though","found","abl",
"quit","sinc","attempt","lack","seen","awar",
nouns <- sapply(txt, extractNoun, USE.NAMES = F)
nouns
nouns <- sapply(txt, extractNoun, USE.NAMES = T)
nouns
nouns <- sapply(txt, extractNoun, USE.NAMES = F)
nouns
head(unlist(nouns), 30)
tail(unlist(nouns), 30)
nouns2 <- unlist(nouns)
nouns <- Filter(function(x) {nchar(x) >= 2}, nouns2)
nouns
nouns <- gsub("☞", "", nouns)
nouns
write(unlist(nouns), "test2.txt" )
table_data <- read.table("test2.txt")
table_data
aggregate(table_data$V1, FUN=sum )
aggregate(table_data$V1, by = list(table_data$V1),  list(FUN=sum )
)
aggregate(table_data$V1, by = list(table_data$V1),FUN=sum )
nouns
ntoken(nouns)
aggregate(nouns, by=list*nouns), FUN=sum)
aggregate(nouns, by=list(nouns), FUN=sum)
aggregate(nouns, by=list(nouns), FUN=sum)
wordcount <- table(unlist(nouns))
wordcount
pal <- brewer.pal(12,"Set3")
pal <- pal[-c(1:2)]
library(RColorBrewer)
library(wordcloud)
pal <- brewer.pal(12,"Set3")
pal <- pal[-c(1:2)]
wordcloud(names(wordcount),freq=wordcount,scale=c(6,0.3),min.freq=25,
random.order=T,rot.per=.1,colors=pal)
pal <- brewer.pal(12,"Set3")
pal <- pal[-c(1:2)]
wordcloud(names(wordcount),freq=wordcount,scale=c(6,0.3),min.freq=3,
random.order=T,rot.per=.1,colors=pal)
f <- file("test33.txt", blocking = F, encoding = "UTF-8")
txt <- readLines(f)
